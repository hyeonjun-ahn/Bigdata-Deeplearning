{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "과제#4_G202038004_안현준.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yNxJAhxavJuz",
        "outputId": "58763f2d-2807-4a9e-d166-262dbdb9c3a4"
      },
      "source": [
        "# If You use in Colab, You Should run this script\n",
        "import os\n",
        "if (not os.path.exists(\"./deep-learning-from-scratch-2\") and\n",
        "    not \"deep-learning-from-scratch-2\" in os.getcwd()):\n",
        "    !git clone  https://github.com/WegraLee/deep-learning-from-scratch-2.git\n",
        "    os.chdir(\"./deep-learning-from-scratch-2\")\n",
        "# !pip install wget"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'deep-learning-from-scratch-2'...\n",
            "remote: Enumerating objects: 598, done.\u001b[K\n",
            "remote: Total 598 (delta 0), reused 0 (delta 0), pack-reused 598\u001b[K\n",
            "Receiving objects: 100% (598/598), 29.81 MiB | 31.34 MiB/s, done.\n",
            "Resolving deltas: 100% (360/360), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYtQbMSjvLJz",
        "outputId": "14feda05-1b90-4452-f08b-afb430557e2c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('./drive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at ./drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guvpuuz9vLLn",
        "outputId": "e70bb785-c78c-45f6-f266-5cda6bceafbc"
      },
      "source": [
        "cd \"/content/drive/MyDrive\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: '/content/drive/MyDrive'\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gu7Wd0W6ImT"
      },
      "source": [
        "#seq2seq"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkKP8lmv5kW7"
      },
      "source": [
        "# Encoder 클래스 코드\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "from common.time_layers import *\n",
        "from common.base_model import BaseModel\n",
        "\n",
        "class Encoder:\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        rn = np.random.randn\n",
        "\n",
        "        embed_W = (rn(V, D) / 100).astype('f')\n",
        "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
        "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
        "        lstm_b = np.zeros(4 * H).astype('f')\n",
        "\n",
        "        self.embed = TimeEmbedding(embed_W)\n",
        "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=False)\n",
        "\n",
        "        self.params = self.embed.params + self.lstm.params\n",
        "        self.grads = self.embed.grads + self.lstm.grads\n",
        "        self.hs = None\n",
        "\n",
        "    def forward(self, xs):\n",
        "        xs = self.embed.forward(xs)\n",
        "        hs = self.lstm.forward(xs)\n",
        "        self.hs = hs\n",
        "        return hs[:, -1, :]\n",
        "\n",
        "    def backward(self, dh):\n",
        "        dhs = np.zeros_like(self.hs)\n",
        "        dhs[:, -1, :] = dh\n",
        "\n",
        "        dout = self.lstm.backward(dhs)\n",
        "        dout = self.embed.backward(dout)\n",
        "        return dout\n",
        "\n",
        "# Decoder 클래스 코드\n",
        "class Decoder:\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        rn = np.random.randn\n",
        "\n",
        "        embed_W = (rn(V, D) / 100).astype('f')\n",
        "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
        "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
        "        lstm_b = np.zeros(4 * H).astype('f')\n",
        "        affine_W = (rn(H, V) / np.sqrt(H)).astype('f')\n",
        "        affine_b = np.zeros(V).astype('f')\n",
        "\n",
        "        self.embed = TimeEmbedding(embed_W)\n",
        "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
        "        self.affine = TimeAffine(affine_W, affine_b)\n",
        "\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in (self.embed, self.lstm, self.affine):\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "    def forward(self, xs, h):\n",
        "        self.lstm.set_state(h)\n",
        "\n",
        "        out = self.embed.forward(xs)\n",
        "        out = self.lstm.forward(out)\n",
        "        score = self.affine.forward(out)\n",
        "        return score\n",
        "\n",
        "    def backward(self, dscore):\n",
        "        dout = self.affine.backward(dscore)\n",
        "        dout = self.lstm.backward(dout)\n",
        "        dout = self.embed.backward(dout)\n",
        "        dh = self.lstm.dh\n",
        "        return dh\n",
        "    \n",
        "    # Decoder 클래스에 문장 생성을 담당하는 generte() 메서드\n",
        "    def generate(self, h, start_id, sample_size):\n",
        "        sampled = []\n",
        "        sample_id = start_id\n",
        "        self.lstm.set_state(h)\n",
        "\n",
        "        for _ in range(sample_size):\n",
        "            x = np.array(sample_id).reshape((1, 1))\n",
        "            out = self.embed.forward(x)\n",
        "            out = self.lstm.forward(out)\n",
        "            score = self.affine.forward(out)\n",
        "\n",
        "            sample_id = np.argmax(score.flatten())\n",
        "            sampled.append(int(sample_id))\n",
        "\n",
        "        return sampled\n",
        "\n",
        "class Seq2seq(BaseModel):\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        self.encoder = Encoder(V, D, H)\n",
        "        self.decoder = Decoder(V, D, H)\n",
        "        self.softmax = TimeSoftmaxWithLoss()\n",
        "\n",
        "        self.params = self.encoder.params + self.decoder.params\n",
        "        self.grads = self.encoder.grads + self.decoder.grads\n",
        "\n",
        "    def forward(self, xs, ts):\n",
        "        decoder_xs, decoder_ts = ts[:, :-1], ts[:, 1:]\n",
        "\n",
        "        h = self.encoder.forward(xs)\n",
        "        score = self.decoder.forward(decoder_xs, h)\n",
        "        loss = self.softmax.forward(score, decoder_ts)\n",
        "        return loss\n",
        "\n",
        "    def backward(self, dout=1):\n",
        "        dout = self.softmax.backward(dout)\n",
        "        dh = self.decoder.backward(dout)\n",
        "        dout = self.encoder.backward(dh)\n",
        "        return dout\n",
        "\n",
        "    def generate(self, xs, start_id, sample_size):\n",
        "        h = self.encoder.forward(xs)\n",
        "        sampled = self.decoder.generate(h, start_id, sample_size)\n",
        "        return sampled\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ej4D4Ptb6dJ4"
      },
      "source": [
        "#seq2seq2 + peeky"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YUh_AzT5kcl"
      },
      "source": [
        "class PeekyDecoder:\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        rn = np.random.randn\n",
        "\n",
        "        embed_W = (rn(V, D) / 100).astype('f')\n",
        "        lstm_Wx = (rn(H + D, 4 * H) / np.sqrt(H + D)).astype('f')\n",
        "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
        "        lstm_b = np.zeros(4 * H).astype('f')\n",
        "        affine_W = (rn(H + H, V) / np.sqrt(H + H)).astype('f')\n",
        "        affine_b = np.zeros(V).astype('f')\n",
        "\n",
        "        self.embed = TimeEmbedding(embed_W)\n",
        "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
        "        self.affine = TimeAffine(affine_W, affine_b)\n",
        "\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in (self.embed, self.lstm, self.affine):\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "        self.cache = None\n",
        "\n",
        "    def forward(self, xs, h):\n",
        "        N, T = xs.shape\n",
        "        N, H = h.shape\n",
        "\n",
        "        self.lstm.set_state(h)\n",
        "\n",
        "        out = self.embed.forward(xs)\n",
        "        hs = np.repeat(h, T, axis=0).reshape(N, T, H)\n",
        "        out = np.concatenate((hs, out), axis=2)\n",
        "\n",
        "        out = self.lstm.forward(out)\n",
        "        out = np.concatenate((hs, out), axis=2)\n",
        "\n",
        "        score = self.affine.forward(out)\n",
        "        self.cache = H\n",
        "        return score\n",
        "\n",
        "    def backward(self, dscore):\n",
        "        H = self.cache\n",
        "\n",
        "        dout = self.affine.backward(dscore)\n",
        "        dout, dhs0 = dout[:, :, H:], dout[:, :, :H]\n",
        "        dout = self.lstm.backward(dout)\n",
        "        dembed, dhs1 = dout[:, :, H:], dout[:, :, :H]\n",
        "        self.embed.backward(dembed)\n",
        "\n",
        "        dhs = dhs0 + dhs1\n",
        "        dh = self.lstm.dh + np.sum(dhs, axis=1)\n",
        "        return dh\n",
        "\n",
        "    def generate(self, h, start_id, sample_size):\n",
        "        sampled = []\n",
        "        char_id = start_id\n",
        "        self.lstm.set_state(h)\n",
        "\n",
        "        H = h.shape[1]\n",
        "        peeky_h = h.reshape(1, 1, H)\n",
        "        for _ in range(sample_size):\n",
        "            x = np.array([char_id]).reshape((1, 1))\n",
        "            out = self.embed.forward(x)\n",
        "\n",
        "            out = np.concatenate((peeky_h, out), axis=2)\n",
        "            out = self.lstm.forward(out)\n",
        "            out = np.concatenate((peeky_h, out), axis=2)\n",
        "            score = self.affine.forward(out)\n",
        "\n",
        "            char_id = np.argmax(score.flatten())\n",
        "            sampled.append(char_id)\n",
        "\n",
        "        return sampled\n",
        "\n",
        "class PeekySeq2seq(Seq2seq):\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        self.encoder = Encoder(V, D, H)\n",
        "        self.decoder = PeekyDecoder(V, D, H)\n",
        "        self.softmax = TimeSoftmaxWithLoss()\n",
        "\n",
        "        self.params = self.encoder.params + self.decoder.params\n",
        "        self.grads = self.encoder.grads + self.decoder.grads"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dcrjJOZ6mB6"
      },
      "source": [
        "#attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-CrQ7tk3XeW"
      },
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "from common.time_layers import *\n",
        "from ch07.seq2seq import Encoder, Seq2seq\n",
        "from ch08.attention_layer import TimeAttention\n",
        "\n",
        "\n",
        "class AttentionEncoder(Encoder):\n",
        "    def forward(self, xs):\n",
        "        xs = self.embed.forward(xs)\n",
        "        hs = self.lstm.forward(xs)\n",
        "        return hs\n",
        "\n",
        "    def backward(self, dhs):\n",
        "        dout = self.lstm.backward(dhs)\n",
        "        dout = self.embed.backward(dout)\n",
        "        return dout\n",
        "\n",
        "\n",
        "class AttentionDecoder:\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "        V, D, H = vocab_size, wordvec_size, hidden_size\n",
        "        rn = np.random.randn\n",
        "\n",
        "        embed_W = (rn(V, D) / 100).astype('f')\n",
        "        lstm_Wx = (rn(D, 4 * H) / np.sqrt(D)).astype('f')\n",
        "        lstm_Wh = (rn(H, 4 * H) / np.sqrt(H)).astype('f')\n",
        "        lstm_b = np.zeros(4 * H).astype('f')\n",
        "        affine_W = (rn(2*H, V) / np.sqrt(2*H)).astype('f')\n",
        "        affine_b = np.zeros(V).astype('f')\n",
        "\n",
        "        self.embed = TimeEmbedding(embed_W)\n",
        "        self.lstm = TimeLSTM(lstm_Wx, lstm_Wh, lstm_b, stateful=True)\n",
        "        self.attention = TimeAttention()\n",
        "        self.affine = TimeAffine(affine_W, affine_b)\n",
        "        layers = [self.embed, self.lstm, self.attention, self.affine]\n",
        "\n",
        "        self.params, self.grads = [], []\n",
        "        for layer in layers:\n",
        "            self.params += layer.params\n",
        "            self.grads += layer.grads\n",
        "\n",
        "    def forward(self, xs, enc_hs):\n",
        "        h = enc_hs[:,-1]\n",
        "        self.lstm.set_state(h)\n",
        "\n",
        "        out = self.embed.forward(xs)\n",
        "        dec_hs = self.lstm.forward(out)\n",
        "        c = self.attention.forward(enc_hs, dec_hs)\n",
        "        out = np.concatenate((c, dec_hs), axis=2)\n",
        "        score = self.affine.forward(out)\n",
        "\n",
        "        return score\n",
        "\n",
        "    def backward(self, dscore):\n",
        "        dout = self.affine.backward(dscore)\n",
        "        N, T, H2 = dout.shape\n",
        "        H = H2 // 2\n",
        "\n",
        "        dc, ddec_hs0 = dout[:,:,:H], dout[:,:,H:]\n",
        "        denc_hs, ddec_hs1 = self.attention.backward(dc)\n",
        "        ddec_hs = ddec_hs0 + ddec_hs1\n",
        "        dout = self.lstm.backward(ddec_hs)\n",
        "        dh = self.lstm.dh\n",
        "        denc_hs[:, -1] += dh\n",
        "        self.embed.backward(dout)\n",
        "\n",
        "        return denc_hs\n",
        "\n",
        "    def generate(self, enc_hs, start_id, sample_size):\n",
        "        sampled = []\n",
        "        sample_id = start_id\n",
        "        h = enc_hs[:, -1]\n",
        "        self.lstm.set_state(h)\n",
        "\n",
        "        for _ in range(sample_size):\n",
        "            x = np.array([sample_id]).reshape((1, 1))\n",
        "\n",
        "            out = self.embed.forward(x)\n",
        "            dec_hs = self.lstm.forward(out)\n",
        "            c = self.attention.forward(enc_hs, dec_hs)\n",
        "            out = np.concatenate((c, dec_hs), axis=2)\n",
        "            score = self.affine.forward(out)\n",
        "\n",
        "            sample_id = np.argmax(score.flatten())\n",
        "            sampled.append(sample_id)\n",
        "\n",
        "        return sampled\n",
        "\n",
        "\n",
        "class AttentionSeq2seq(Seq2seq):\n",
        "    def __init__(self, vocab_size, wordvec_size, hidden_size):\n",
        "        args = vocab_size, wordvec_size, hidden_size\n",
        "        self.encoder = AttentionEncoder(*args)\n",
        "        self.decoder = AttentionDecoder(*args)\n",
        "        self.softmax = TimeSoftmaxWithLoss()\n",
        "\n",
        "        self.params = self.encoder.params + self.decoder.params\n",
        "        self.grads = self.encoder.grads + self.decoder.grads"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8W8K3Ua26tw4"
      },
      "source": [
        "#학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28ckYb9I4OJT",
        "outputId": "45351297-650c-46c3-8cd0-54be64ccfe04"
      },
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "sys.path.append('../ch07')\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from dataset import sequence\n",
        "from common.optimizer import Adam\n",
        "from common.trainer import Trainer\n",
        "from common.util import eval_seq2seq\n",
        "\n",
        "# 데이터 읽기\n",
        "(x_train, t_train), (x_test, t_test) = sequence.load_data('date.txt')\n",
        "char_to_id, id_to_char = sequence.get_vocab()\n",
        "\n",
        "# 입력 문장 반전\n",
        "x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\n",
        "\n",
        "# 하이퍼파라미터 설정\n",
        "vocab_size = len(char_to_id)\n",
        "wordvec_size = 16\n",
        "hidden_size = 256\n",
        "batch_size = 128\n",
        "max_epoch = 10\n",
        "max_grad = 5.0\n",
        "\n",
        "#----------------------------------------------------------------#\n",
        "#model = AttentionSeq2seq(vocab_size, wordvec_size, hidden_size)\n",
        "#model = Seq2seq(vocab_size, wordvec_size, hidden_size)\n",
        "model = PeekySeq2seq(vocab_size, wordvec_size, hidden_size)\n",
        "#----------------------------------------------------------------#\n",
        "\n",
        "optimizer = Adam()\n",
        "trainer = Trainer(model, optimizer)\n",
        "#----------------------------#\n",
        "#acc_list = []\n",
        "#acc_list_seq = []\n",
        "acc_list_peeky = []\n",
        "#----------------------------#\n",
        "for epoch in range(max_epoch):\n",
        "    trainer.fit(x_train, t_train, max_epoch=1,\n",
        "                batch_size=batch_size, max_grad=max_grad)\n",
        "\n",
        "    correct_num = 0\n",
        "    for i in range(len(x_test)):\n",
        "        question, correct = x_test[[i]], t_test[[i]]\n",
        "        verbose = i < 10\n",
        "        correct_num += eval_seq2seq(model, question, correct,\n",
        "                                    id_to_char, verbose, is_reverse=True)\n",
        "\n",
        "    acc = float(correct_num) / len(x_test)\n",
        "    #----------------------------#\n",
        "    #acc_list.append(acc)\n",
        "    #acc_list_seq.append(acc)\n",
        "    acc_list_peeky.append(acc)\n",
        "    #----------------------------#\n",
        "    print('정확도 %.3f%%' % (acc * 100))\n",
        "\n",
        "\n",
        "model.save_params()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| 에폭 1 |  반복 1 / 351 | 시간 0[s] | 손실 4.08\n",
            "| 에폭 1 |  반복 21 / 351 | 시간 12[s] | 손실 2.86\n",
            "| 에폭 1 |  반복 41 / 351 | 시간 24[s] | 손실 1.89\n",
            "| 에폭 1 |  반복 61 / 351 | 시간 37[s] | 손실 1.78\n",
            "| 에폭 1 |  반복 81 / 351 | 시간 49[s] | 손실 1.70\n",
            "| 에폭 1 |  반복 101 / 351 | 시간 61[s] | 손실 1.57\n",
            "| 에폭 1 |  반복 121 / 351 | 시간 74[s] | 손실 1.30\n",
            "| 에폭 1 |  반복 141 / 351 | 시간 86[s] | 손실 1.16\n",
            "| 에폭 1 |  반복 161 / 351 | 시간 98[s] | 손실 1.10\n",
            "| 에폭 1 |  반복 181 / 351 | 시간 110[s] | 손실 1.07\n",
            "| 에폭 1 |  반복 201 / 351 | 시간 122[s] | 손실 1.05\n",
            "| 에폭 1 |  반복 221 / 351 | 시간 135[s] | 손실 1.04\n",
            "| 에폭 1 |  반복 241 / 351 | 시간 147[s] | 손실 1.04\n",
            "| 에폭 1 |  반복 261 / 351 | 시간 159[s] | 손실 1.03\n",
            "| 에폭 1 |  반복 281 / 351 | 시간 172[s] | 손실 1.02\n",
            "| 에폭 1 |  반복 301 / 351 | 시간 184[s] | 손실 1.01\n",
            "| 에폭 1 |  반복 321 / 351 | 시간 196[s] | 손실 1.00\n",
            "| 에폭 1 |  반복 341 / 351 | 시간 209[s] | 손실 1.00\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[91m☒\u001b[0m 1971-11-11\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[91m☒\u001b[0m 1973-01-11\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[91m☒\u001b[0m 1983-03-03\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[91m☒\u001b[0m 1973-01-11\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[91m☒\u001b[0m 1973-04-09\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[91m☒\u001b[0m 1971-11-11\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[91m☒\u001b[0m 1983-04-09\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[91m☒\u001b[0m 1983-04-09\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[91m☒\u001b[0m 1971-11-11\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[91m☒\u001b[0m 1996-01-11\n",
            "---\n",
            "정확도 0.020%\n",
            "| 에폭 2 |  반복 1 / 351 | 시간 0[s] | 손실 0.98\n",
            "| 에폭 2 |  반복 21 / 351 | 시간 13[s] | 손실 0.98\n",
            "| 에폭 2 |  반복 41 / 351 | 시간 25[s] | 손실 0.97\n",
            "| 에폭 2 |  반복 61 / 351 | 시간 37[s] | 손실 0.96\n",
            "| 에폭 2 |  반복 81 / 351 | 시간 50[s] | 손실 0.96\n",
            "| 에폭 2 |  반복 101 / 351 | 시간 63[s] | 손실 0.95\n",
            "| 에폭 2 |  반복 121 / 351 | 시간 75[s] | 손실 0.93\n",
            "| 에폭 2 |  반복 141 / 351 | 시간 88[s] | 손실 0.92\n",
            "| 에폭 2 |  반복 161 / 351 | 시간 100[s] | 손실 0.91\n",
            "| 에폭 2 |  반복 181 / 351 | 시간 113[s] | 손실 0.90\n",
            "| 에폭 2 |  반복 201 / 351 | 시간 125[s] | 손실 0.89\n",
            "| 에폭 2 |  반복 221 / 351 | 시간 138[s] | 손실 0.88\n",
            "| 에폭 2 |  반복 241 / 351 | 시간 150[s] | 손실 0.87\n",
            "| 에폭 2 |  반복 261 / 351 | 시간 162[s] | 손실 0.85\n",
            "| 에폭 2 |  반복 281 / 351 | 시간 175[s] | 손실 0.83\n",
            "| 에폭 2 |  반복 301 / 351 | 시간 188[s] | 손실 0.81\n",
            "| 에폭 2 |  반복 321 / 351 | 시간 200[s] | 손실 0.81\n",
            "| 에폭 2 |  반복 341 / 351 | 시간 213[s] | 손실 0.76\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[91m☒\u001b[0m 1974-11-04\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[91m☒\u001b[0m 2011-01-11\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[91m☒\u001b[0m 2003-03-03\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[91m☒\u001b[0m 2011-10-11\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[91m☒\u001b[0m 1971-09-11\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[91m☒\u001b[0m 2001-10-12\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[91m☒\u001b[0m 1992-04-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[91m☒\u001b[0m 1970-07-07\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[91m☒\u001b[0m 2006-10-27\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[91m☒\u001b[0m 2011-10-11\n",
            "---\n",
            "정확도 0.420%\n",
            "| 에폭 3 |  반복 1 / 351 | 시간 0[s] | 손실 0.73\n",
            "| 에폭 3 |  반복 21 / 351 | 시간 13[s] | 손실 0.74\n",
            "| 에폭 3 |  반복 41 / 351 | 시간 25[s] | 손실 0.71\n",
            "| 에폭 3 |  반복 61 / 351 | 시간 37[s] | 손실 0.69\n",
            "| 에폭 3 |  반복 81 / 351 | 시간 48[s] | 손실 0.67\n",
            "| 에폭 3 |  반복 101 / 351 | 시간 60[s] | 손실 0.63\n",
            "| 에폭 3 |  반복 121 / 351 | 시간 71[s] | 손실 0.60\n",
            "| 에폭 3 |  반복 141 / 351 | 시간 83[s] | 손실 0.59\n",
            "| 에폭 3 |  반복 161 / 351 | 시간 94[s] | 손실 0.55\n",
            "| 에폭 3 |  반복 181 / 351 | 시간 106[s] | 손실 0.53\n",
            "| 에폭 3 |  반복 201 / 351 | 시간 117[s] | 손실 0.50\n",
            "| 에폭 3 |  반복 221 / 351 | 시간 129[s] | 손실 0.48\n",
            "| 에폭 3 |  반복 241 / 351 | 시간 141[s] | 손실 0.45\n",
            "| 에폭 3 |  반복 261 / 351 | 시간 153[s] | 손실 0.42\n",
            "| 에폭 3 |  반복 281 / 351 | 시간 164[s] | 손실 0.40\n",
            "| 에폭 3 |  반복 301 / 351 | 시간 175[s] | 손실 0.37\n",
            "| 에폭 3 |  반복 321 / 351 | 시간 186[s] | 손실 0.34\n",
            "| 에폭 3 |  반복 341 / 351 | 시간 198[s] | 손실 0.31\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[91m☒\u001b[0m 1985-10-13\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[92m☑\u001b[0m 2008-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[91m☒\u001b[0m 2016-10-21\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[91m☒\u001b[0m 1970-07-17\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[91m☒\u001b[0m 1980-08-20\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[91m☒\u001b[0m 2007-08-06\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[91m☒\u001b[0m 2015-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[92m☑\u001b[0m 2016-11-06\n",
            "---\n",
            "정확도 28.980%\n",
            "| 에폭 4 |  반복 1 / 351 | 시간 0[s] | 손실 0.29\n",
            "| 에폭 4 |  반복 21 / 351 | 시간 11[s] | 손실 0.27\n",
            "| 에폭 4 |  반복 41 / 351 | 시간 23[s] | 손실 0.24\n",
            "| 에폭 4 |  반복 61 / 351 | 시간 34[s] | 손실 0.23\n",
            "| 에폭 4 |  반복 81 / 351 | 시간 45[s] | 손실 0.21\n",
            "| 에폭 4 |  반복 101 / 351 | 시간 56[s] | 손실 0.19\n",
            "| 에폭 4 |  반복 121 / 351 | 시간 68[s] | 손실 0.17\n",
            "| 에폭 4 |  반복 141 / 351 | 시간 79[s] | 손실 0.15\n",
            "| 에폭 4 |  반복 161 / 351 | 시간 91[s] | 손실 0.13\n",
            "| 에폭 4 |  반복 181 / 351 | 시간 103[s] | 손실 0.12\n",
            "| 에폭 4 |  반복 201 / 351 | 시간 115[s] | 손실 0.12\n",
            "| 에폭 4 |  반복 221 / 351 | 시간 127[s] | 손실 0.10\n",
            "| 에폭 4 |  반복 241 / 351 | 시간 139[s] | 손실 0.09\n",
            "| 에폭 4 |  반복 261 / 351 | 시간 151[s] | 손실 0.08\n",
            "| 에폭 4 |  반복 281 / 351 | 시간 163[s] | 손실 0.07\n",
            "| 에폭 4 |  반복 301 / 351 | 시간 174[s] | 손실 0.07\n",
            "| 에폭 4 |  반복 321 / 351 | 시간 186[s] | 손실 0.06\n",
            "| 에폭 4 |  반복 341 / 351 | 시간 198[s] | 손실 0.06\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[92m☑\u001b[0m 2008-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[92m☑\u001b[0m 2007-08-30\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[92m☑\u001b[0m 2013-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[92m☑\u001b[0m 2016-11-06\n",
            "---\n",
            "정확도 92.480%\n",
            "| 에폭 5 |  반복 1 / 351 | 시간 0[s] | 손실 0.05\n",
            "| 에폭 5 |  반복 21 / 351 | 시간 11[s] | 손실 0.04\n",
            "| 에폭 5 |  반복 41 / 351 | 시간 23[s] | 손실 0.04\n",
            "| 에폭 5 |  반복 61 / 351 | 시간 34[s] | 손실 0.03\n",
            "| 에폭 5 |  반복 81 / 351 | 시간 45[s] | 손실 0.03\n",
            "| 에폭 5 |  반복 101 / 351 | 시간 57[s] | 손실 0.03\n",
            "| 에폭 5 |  반복 121 / 351 | 시간 68[s] | 손실 0.03\n",
            "| 에폭 5 |  반복 141 / 351 | 시간 79[s] | 손실 0.02\n",
            "| 에폭 5 |  반복 161 / 351 | 시간 90[s] | 손실 0.02\n",
            "| 에폭 5 |  반복 181 / 351 | 시간 101[s] | 손실 0.02\n",
            "| 에폭 5 |  반복 201 / 351 | 시간 112[s] | 손실 0.02\n",
            "| 에폭 5 |  반복 221 / 351 | 시간 124[s] | 손실 0.02\n",
            "| 에폭 5 |  반복 241 / 351 | 시간 135[s] | 손실 0.02\n",
            "| 에폭 5 |  반복 261 / 351 | 시간 146[s] | 손실 0.01\n",
            "| 에폭 5 |  반복 281 / 351 | 시간 157[s] | 손실 0.01\n",
            "| 에폭 5 |  반복 301 / 351 | 시간 168[s] | 손실 0.01\n",
            "| 에폭 5 |  반복 321 / 351 | 시간 180[s] | 손실 0.01\n",
            "| 에폭 5 |  반복 341 / 351 | 시간 191[s] | 손실 0.01\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[92m☑\u001b[0m 2008-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[92m☑\u001b[0m 2007-08-30\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[92m☑\u001b[0m 2013-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[92m☑\u001b[0m 2016-11-06\n",
            "---\n",
            "정확도 99.760%\n",
            "| 에폭 6 |  반복 1 / 351 | 시간 0[s] | 손실 0.01\n",
            "| 에폭 6 |  반복 21 / 351 | 시간 11[s] | 손실 0.01\n",
            "| 에폭 6 |  반복 41 / 351 | 시간 22[s] | 손실 0.01\n",
            "| 에폭 6 |  반복 61 / 351 | 시간 33[s] | 손실 0.01\n",
            "| 에폭 6 |  반복 81 / 351 | 시간 44[s] | 손실 0.01\n",
            "| 에폭 6 |  반복 101 / 351 | 시간 55[s] | 손실 0.01\n",
            "| 에폭 6 |  반복 121 / 351 | 시간 66[s] | 손실 0.01\n",
            "| 에폭 6 |  반복 141 / 351 | 시간 77[s] | 손실 0.01\n",
            "| 에폭 6 |  반복 161 / 351 | 시간 88[s] | 손실 0.01\n",
            "| 에폭 6 |  반복 181 / 351 | 시간 99[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 201 / 351 | 시간 110[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 221 / 351 | 시간 121[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 241 / 351 | 시간 132[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 261 / 351 | 시간 143[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 281 / 351 | 시간 154[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 301 / 351 | 시간 166[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 321 / 351 | 시간 177[s] | 손실 0.00\n",
            "| 에폭 6 |  반복 341 / 351 | 시간 188[s] | 손실 0.00\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[92m☑\u001b[0m 2008-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[92m☑\u001b[0m 2007-08-30\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[92m☑\u001b[0m 2013-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[92m☑\u001b[0m 2016-11-06\n",
            "---\n",
            "정확도 100.000%\n",
            "| 에폭 7 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 21 / 351 | 시간 11[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 41 / 351 | 시간 23[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 61 / 351 | 시간 34[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 81 / 351 | 시간 45[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 101 / 351 | 시간 56[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 121 / 351 | 시간 68[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 141 / 351 | 시간 79[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 161 / 351 | 시간 90[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 181 / 351 | 시간 102[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 201 / 351 | 시간 114[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 221 / 351 | 시간 126[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 241 / 351 | 시간 138[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 261 / 351 | 시간 150[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 281 / 351 | 시간 162[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 301 / 351 | 시간 174[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 321 / 351 | 시간 185[s] | 손실 0.00\n",
            "| 에폭 7 |  반복 341 / 351 | 시간 197[s] | 손실 0.00\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[92m☑\u001b[0m 2008-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[92m☑\u001b[0m 2007-08-30\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[92m☑\u001b[0m 2013-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[92m☑\u001b[0m 2016-11-06\n",
            "---\n",
            "정확도 100.000%\n",
            "| 에폭 8 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 21 / 351 | 시간 11[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 41 / 351 | 시간 23[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 61 / 351 | 시간 34[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 81 / 351 | 시간 46[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 101 / 351 | 시간 57[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 121 / 351 | 시간 68[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 141 / 351 | 시간 79[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 161 / 351 | 시간 90[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 181 / 351 | 시간 102[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 201 / 351 | 시간 113[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 221 / 351 | 시간 124[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 241 / 351 | 시간 136[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 261 / 351 | 시간 147[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 281 / 351 | 시간 158[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 301 / 351 | 시간 169[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 321 / 351 | 시간 181[s] | 손실 0.00\n",
            "| 에폭 8 |  반복 341 / 351 | 시간 192[s] | 손실 0.00\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[92m☑\u001b[0m 2008-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[92m☑\u001b[0m 2007-08-30\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[92m☑\u001b[0m 2013-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[92m☑\u001b[0m 2016-11-06\n",
            "---\n",
            "정확도 100.000%\n",
            "| 에폭 9 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 21 / 351 | 시간 12[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 41 / 351 | 시간 23[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 61 / 351 | 시간 35[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 81 / 351 | 시간 47[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 101 / 351 | 시간 59[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 121 / 351 | 시간 71[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 141 / 351 | 시간 84[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 161 / 351 | 시간 96[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 181 / 351 | 시간 108[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 201 / 351 | 시간 121[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 221 / 351 | 시간 133[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 241 / 351 | 시간 146[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 261 / 351 | 시간 158[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 281 / 351 | 시간 171[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 301 / 351 | 시간 183[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 321 / 351 | 시간 195[s] | 손실 0.00\n",
            "| 에폭 9 |  반복 341 / 351 | 시간 207[s] | 손실 0.00\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[92m☑\u001b[0m 2008-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[92m☑\u001b[0m 2007-08-30\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[92m☑\u001b[0m 2013-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[92m☑\u001b[0m 2016-11-06\n",
            "---\n",
            "정확도 100.000%\n",
            "| 에폭 10 |  반복 1 / 351 | 시간 0[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 21 / 351 | 시간 12[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 41 / 351 | 시간 25[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 61 / 351 | 시간 37[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 81 / 351 | 시간 49[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 101 / 351 | 시간 62[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 121 / 351 | 시간 74[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 141 / 351 | 시간 86[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 161 / 351 | 시간 98[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 181 / 351 | 시간 110[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 201 / 351 | 시간 122[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 221 / 351 | 시간 134[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 241 / 351 | 시간 147[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 261 / 351 | 시간 159[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 281 / 351 | 시간 171[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 301 / 351 | 시간 183[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 321 / 351 | 시간 195[s] | 손실 0.00\n",
            "| 에폭 10 |  반복 341 / 351 | 시간 207[s] | 손실 0.00\n",
            "Q 10/15/94                     \n",
            "T 1994-10-15\n",
            "\u001b[92m☑\u001b[0m 1994-10-15\n",
            "---\n",
            "Q thursday, november 13, 2008  \n",
            "T 2008-11-13\n",
            "\u001b[92m☑\u001b[0m 2008-11-13\n",
            "---\n",
            "Q Mar 25, 2003                 \n",
            "T 2003-03-25\n",
            "\u001b[92m☑\u001b[0m 2003-03-25\n",
            "---\n",
            "Q Tuesday, November 22, 2016   \n",
            "T 2016-11-22\n",
            "\u001b[92m☑\u001b[0m 2016-11-22\n",
            "---\n",
            "Q Saturday, July 18, 1970      \n",
            "T 1970-07-18\n",
            "\u001b[92m☑\u001b[0m 1970-07-18\n",
            "---\n",
            "Q october 6, 1992              \n",
            "T 1992-10-06\n",
            "\u001b[92m☑\u001b[0m 1992-10-06\n",
            "---\n",
            "Q 8/23/08                      \n",
            "T 2008-08-23\n",
            "\u001b[92m☑\u001b[0m 2008-08-23\n",
            "---\n",
            "Q 8/30/07                      \n",
            "T 2007-08-30\n",
            "\u001b[92m☑\u001b[0m 2007-08-30\n",
            "---\n",
            "Q 10/28/13                     \n",
            "T 2013-10-28\n",
            "\u001b[92m☑\u001b[0m 2013-10-28\n",
            "---\n",
            "Q sunday, november 6, 2016     \n",
            "T 2016-11-06\n",
            "\u001b[92m☑\u001b[0m 2016-11-06\n",
            "---\n",
            "정확도 100.000%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "jnE79_FR7IJ7",
        "outputId": "40f5ea43-f50a-471b-dad2-8f51a00212af"
      },
      "source": [
        "# 그래프 그리기\n",
        "x = np.arange(len(acc_list))\n",
        "plt.plot(x, acc_list, marker='o', label='attention')\n",
        "plt.plot(x, acc_list_seq, marker='s', label='seq2seq')\n",
        "plt.plot(x, acc_list_peeky, marker='^', label='seq2seq+peeky')\n",
        "plt.legend(loc=0)\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.ylim(-0.05, 1.05)\n",
        "plt.show()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xusMJdSp7S0T"
      },
      "source": [
        "# acc_list : attention\n",
        "# acc_list_seq : seq2seq\n",
        "# acc_list_peeky : seq2seq + peeky"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}